import logging
from typing import List, Dict, AsyncGenerator, Optional
import asyncio

from langchain.schema import HumanMessage, SystemMessage, AIMessage, BaseMessage
import json

from .utils import stream_chat_response
from .chains import stream_route_and_respond

logger = logging.getLogger('django.llm.agent')


def _normalize_history(history: Optional[List[Dict]]) -> List[BaseMessage]:
    """
    å°†å‰ç«¯ä¼ å…¥çš„èŠå¤©å†å²è§„èŒƒåŒ–ä¸ºLangChainçš„æ¶ˆæ¯å¯¹è±¡åˆ—è¡¨ã€‚
    å…è®¸çš„è¾“å…¥æ ¼å¼ç¤ºä¾‹ï¼š
    [
        {"role": "system", "content": "ä½ æ˜¯æ ¡å›­æ™ºèƒ½åŠ©æ‰‹"},
        {"role": "user", "content": "ä½ å¥½"},
        {"role": "assistant", "content": "ä½ å¥½ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®ä½ ï¼Ÿ"}
    ]
    """
    messages: List[BaseMessage] = []
    if not history:
        return messages

    for item in history:
        if not isinstance(item, dict):
            continue
        role = (item.get("role") or "").lower()
        content = item.get("content")
        if not content:
            continue
        if role == "system":
            messages.append(SystemMessage(content=content))
        elif role == "assistant":
            messages.append(AIMessage(content=content))
        else:
            messages.append(HumanMessage(content=content))
    return messages


async def get_agent_response(
    user_message: str,
    history: Optional[List[Dict]] = None,
    system_prompt: Optional[str] = None,
    temperature: float = 0.7,
    model_type: Optional[str] = None,
) -> AsyncGenerator[str, None]:
    """
    ä»£ç†å“åº”æ¥å£ï¼š
    - è‹¥æä¾›è‡ªå®šä¹‰ system_promptï¼Œåˆ™èµ°åŸºç¡€æµå¼å¯¹è¯ï¼ˆä¸åŸå®ç°å…¼å®¹ï¼‰ã€‚
    - å¦åˆ™é»˜è®¤èµ°è·¯ç”±é“¾ï¼Œè‡ªåŠ¨è¯†åˆ«æ„å›¾å¹¶è°ƒç”¨å·¥å…·ä¸æç¤ºè¯ã€‚
    å¯é€‰çš„ model_type ç”¨äºæŒ‡å®šç”Ÿæˆæ‰€ä½¿ç”¨çš„æ¨¡å‹ç±»å‹ã€‚
    """
    try:
        if system_prompt:
            # å…¼å®¹æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨system_prompt + å†å² + ç”¨æˆ·è¾“å…¥ è¿›è¡Œæµå¼å¯¹è¯
            msg_list: List[BaseMessage] = []
            msg_list.append(SystemMessage(content=system_prompt))
            msg_list.extend(_normalize_history(history))
            msg_list.append(HumanMessage(content=user_message))

            # ç»Ÿä¸€è¾“å‡ºä¸ºé€è¡ŒJSONäº‹ä»¶ï¼Œé¿å…å‰ç«¯è§£æä¸ä¸€è‡´
            yield json.dumps({
                "type": "chain_start",
                "step": "llm_streaming",
                "message": "ğŸ’¬ æ­£åœ¨ç”Ÿæˆå›ç­”..."
            }, ensure_ascii=False) + "\n"
            await asyncio.sleep(0)

            async for chunk in stream_chat_response(msg_list, temperature=temperature, model_type=(model_type or "fast")):
                yield json.dumps({"type": "content", "text": chunk}, ensure_ascii=False) + "\n"
                await asyncio.sleep(0)

            yield json.dumps({"type": "chain_end", "message": "âœ… å¤„ç†å®Œæˆ"}, ensure_ascii=False) + "\n"
            await asyncio.sleep(0)
            return

        # é»˜è®¤ï¼šèµ°å¸¦å·¥å…·/æç¤ºè¯çš„è·¯ç”±é“¾ï¼ˆå«çœŸæ­£é€æ­¥æµå¼è¾“å‡ºï¼‰
        async for chunk in stream_route_and_respond(user_message, history or [], model_type=model_type):
            yield chunk
    except Exception as e:
        logger.error(f"ç”Ÿæˆä»£ç†å“åº”å¤±è´¥: {str(e)}", exc_info=True)
        yield json.dumps({
            "type": "error",
            "message": f"âŒ ç”Ÿæˆå“åº”å¤±è´¥: {str(e)}"
        }, ensure_ascii=False) + "\n"