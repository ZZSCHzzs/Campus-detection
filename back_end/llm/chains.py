"""
LLM é“¾å¼æ¨ç†æ¨¡å— - AI Agent å¾ªç¯
- å®ç°å®Œæ•´çš„ Agent å¾ªç¯ï¼š"è®¡åˆ’-æ‰§è¡Œ-è§‚å¯Ÿ-å†è®¡åˆ’"
- æ”¯æŒçœŸæ­£çš„æµå¼è¾“å‡ºï¼Œæ¯ä¸ªæ­¥éª¤å®æ—¶ä¼ è¾“
- é›†æˆæ¨¡ç³Šæœç´¢å·¥å…·ï¼Œç¡®ä¿ä¸ç¼–é€ ID
- ä¸å‰ç«¯æ­¥éª¤æ˜¾ç¤ºç»„ä»¶å®Œå…¨å…¼å®¹
"""
from __future__ import annotations

from typing import List, Dict, Any, AsyncGenerator, Optional
import json
import logging
import asyncio

from langchain.schema import SystemMessage, HumanMessage, AIMessage, BaseMessage
from langchain.prompts import ChatPromptTemplate

from .utils import get_llm_client, stream_chat_response, run_llm_with_retry
from .prompts import get_chat_prompt_with_history, get_agent_planner_prompt
from .memory import ChatMemoryStore, normalize_frontend_history
from . import tools as campus_tools
from channels.db import database_sync_to_async

logger = logging.getLogger(__name__)


def build_basic_chat_chain():
    """æ„å»ºåŸºç¡€å¯¹è¯é“¾ï¼ˆä¸å¸¦å·¥å…·ï¼‰"""
    llm = get_llm_client(temperature=0.6, model_type="fast")
    return llm


async def _get_available_tools_spec() -> str:
    """è·å–å¯ç”¨å·¥å…·çš„è§„æ ¼è¯´æ˜"""
    tools_spec = """
**æ¨¡ç³Šæœç´¢å·¥å…·**ï¼š
- fuzzy_search_areas(query: str, category: str | None = None) -> List[Dict]: æ¨¡ç³Šæœç´¢åŒºåŸŸï¼Œæ”¯æŒåŒºåŸŸåã€å»ºç­‘åã€æ¥¼å±‚ç­‰å…³é”®è¯ï¼›å¯é€‰æŒ‰å»ºç­‘ç±»å‹è¿‡æ»¤
- fuzzy_search_buildings(query: str) -> List[Dict]: æ¨¡ç³Šæœç´¢å»ºç­‘
- fuzzy_search_terminals(query: str) -> List[Dict]: æ¨¡ç³Šæœç´¢ç»ˆç«¯

**ç²¾ç¡®æŸ¥è¯¢å·¥å…·**ï¼ˆéœ€è¦ç¡®åˆ‡IDï¼‰:
- get_area_status(area_id: int) -> Dict: è·å–æŒ‡å®šåŒºåŸŸçš„ç»¼åˆçŠ¶æ€ï¼ˆäººæµã€ç¯å¢ƒã€å‘Šè­¦ï¼‰
- get_terminal_status(terminal_id: int) -> Dict: è·å–æŒ‡å®šç»ˆç«¯çš„çŠ¶æ€ï¼ˆCPUã€å†…å­˜ã€ç£ç›˜ï¼‰
- get_suggested_areas(limit: int = 5, category: str | None = None) -> List[Dict]: è·å–æ¨èåŒºåŸŸåˆ—è¡¨ï¼›å¯é€‰æŒ‰å»ºç­‘ç±»å‹è¿‡æ»¤

**å†å²æ•°æ®å·¥å…·**ï¼š
- get_area_trend(area_id: int, hours: int = 6) -> List[Dict]: è·å–åŒºåŸŸäººæµè¶‹åŠ¿
- get_area_extremes(area_id: int, hours: int = 24) -> Dict: è·å–åŒºåŸŸæå€¼æ•°æ®
- get_recent_alerts(area_id: int = None, limit: int = 10) -> List[Dict]: è·å–å‘Šè­¦ä¿¡æ¯
- get_environment_snapshots(area_id: int, hours: int = 6) -> List[Dict]: è·å–ç¯å¢ƒæ•°æ®å¿«ç…§

**èµ„æºå¯¼èˆªå·¥å…·**ï¼š
- get_campus_resources(query: str = "") -> List[Dict]: æ ¹æ®å…³é”®è¯è·å–æ ¡å›­æœåŠ¡èµ„æºé“¾æ¥
- get_general_campus_info() -> Dict: è·å–AIåŠ©æ‰‹èº«ä»½ä¿¡æ¯å’Œæ ¡å›­åŸºç¡€ä¿¡æ¯

ã€é‡è¦ã€‘å·¥å…·æ˜¯å¯é€‰çš„ï¼šå½“ç”¨æˆ·é—®é¢˜ä¸éœ€è¦è°ƒç”¨å·¥å…·ï¼ˆå¦‚é—²èŠã€ä¸€èˆ¬å’¨è¯¢ã€ä¸æ•°æ®æ— å…³çš„é—®ç­”ï¼‰æ—¶ï¼Œè¯·é€‰æ‹© direct_response å¹¶ç›´æ¥ä½œç­”ã€‚
ã€è‡ªç”±é—®ç­”ã€‘å…è®¸å›ç­”ä¸æ ¡å›­æ— å…³çš„é€šç”¨é—®é¢˜ï¼Œä¸è¦å› ä¸ºè¶…å‡ºé¢†åŸŸè€Œæ‹’ç­”ï¼ˆä»…åœ¨å†…å®¹å—é™æ—¶æ‰æ‹’ç»ï¼‰ã€‚

ã€å¯é€‰å»ºç­‘ç±»å‹ï¼ˆcategoryæšä¸¾ï¼‰ã€‘ï¼š
- library, study, teaching, cafeteria, dorm, lab, office, sports, service, other
"""
    return tools_spec


async def _execute_tool(tool_name: str, parameters: Dict[str, Any]) -> Dict[str, Any]:
    """æ‰§è¡Œå…·ä½“çš„å·¥å…·è°ƒç”¨"""
    try:
        if tool_name == "fuzzy_search_areas":
            result = await database_sync_to_async(campus_tools.fuzzy_search_areas, thread_sensitive=False)(
                parameters.get("query", ""), 
                parameters.get("category")
            )
        elif tool_name == "fuzzy_search_buildings":
            result = await database_sync_to_async(campus_tools.fuzzy_search_buildings, thread_sensitive=False)(parameters.get("query", ""))
        elif tool_name == "fuzzy_search_terminals":
            result = await database_sync_to_async(campus_tools.fuzzy_search_terminals, thread_sensitive=False)(parameters.get("query", ""))
        elif tool_name == "get_area_status":
            result = await database_sync_to_async(campus_tools.get_area_status, thread_sensitive=False)(parameters.get("area_id"))
        elif tool_name == "get_terminal_status":
            result = await database_sync_to_async(campus_tools.get_terminal_status, thread_sensitive=False)(parameters.get("terminal_id"))
        elif tool_name == "get_suggested_areas":
            result = await database_sync_to_async(campus_tools.get_suggested_areas, thread_sensitive=False)(
                limit=parameters.get("limit", 5),
                category=parameters.get("category")
            )
        elif tool_name == "get_area_trend":
            result = await database_sync_to_async(campus_tools.get_area_trend, thread_sensitive=False)(
                parameters.get("area_id"), parameters.get("hours", 6)
            )
        elif tool_name == "get_area_extremes":
            result = await database_sync_to_async(campus_tools.get_area_extremes, thread_sensitive=False)(
                parameters.get("area_id"), parameters.get("hours", 24)
            )
        elif tool_name == "get_recent_alerts":
            result = await database_sync_to_async(campus_tools.get_recent_alerts, thread_sensitive=False)(
                parameters.get("area_id"), parameters.get("limit", 10)
            )
        elif tool_name == "get_environment_snapshots":
            result = await database_sync_to_async(campus_tools.get_environment_snapshots, thread_sensitive=False)(
                parameters.get("area_id"), parameters.get("hours", 6)
            )
        elif tool_name == "get_campus_resources":
            result = await database_sync_to_async(campus_tools.get_campus_resources, thread_sensitive=False)(parameters.get("query", ""))
        elif tool_name == "get_general_campus_info":
            result = await database_sync_to_async(campus_tools.get_general_campus_info, thread_sensitive=False)()
        else:
            return {"error": f"Unknown tool: {tool_name}"}
        
        return {"success": True, "result": result}
    except Exception as e:
        logger.error(f"Tool execution error - {tool_name}: {e}")
        return {"error": str(e)}


async def _llm_plan_next_action_streaming(
    user_input: str, 
    history: str, 
    observations: str,
    planner_model: Optional[str] = None
) -> AsyncGenerator[tuple, None]:
    """ä½¿ç”¨æµå¼LLMè§„åˆ’ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼Œå®æ—¶è¾“å‡ºæ€è€ƒè¿‡ç¨‹"""
    tools_spec = await _get_available_tools_spec()
    prompt = get_agent_planner_prompt(user_input, tools_spec, history, observations)
    
    messages = [
        SystemMessage(content="ä½ æ˜¯äº‘å°ç»çš„è§„åˆ’å™¨ï¼Œå¿…é¡»ä»¥æ ‡å‡†JSONæ ¼å¼å›å¤ã€‚è‹¥ç”¨æˆ·é—®é¢˜æ— éœ€å·¥å…·ï¼Œè¯·é€‰æ‹©direct_responseï¼Œä½†ä¸è¦ç›´æ¥è¾“å‡ºå®Œæ•´å›ç­”ï¼›è¯·ä»¥outlineç»™å‡ºå›ç­”å¤§çº²ï¼ˆ3-6æ¡ï¼‰ã€‚å…è®¸è‡ªç”±é—®ç­”ï¼Œä¸é™äºæ ¡å›­é¢†åŸŸï¼›é™¤éæ¶‰åŠå—é™å†…å®¹ï¼Œå¦åˆ™ä¸è¦æ‹’ç»ã€‚"),
        HumanMessage(content=prompt)
    ]
    
    try:
        thinking_content = ""
        final_plan = {}
        
        # ä½¿ç”¨è¾ƒå¿«æ¨¡å‹è¿›è¡Œè§„åˆ’ï¼ˆè‹¥æœªæŒ‡å®šï¼‰
        model_for_planning = planner_model or "fast"

        # å¢é‡ JSON è§£æå™¨çŠ¶æ€ï¼ˆé¿å…æ­£åˆ™ O(n^2) æ‰«æä¸é˜»å¡ï¼‰
        in_string = False
        string_quote = ""
        escape = False
        depth = 0
        started = False
        json_buffer: list[str] = []

        # æµå¼è·å–LLMå“åº”
        async for chunk in stream_chat_response(messages, temperature=0.1, model_type=model_for_planning):
            thinking_content += chunk
            if chunk.strip():
                yield ("thinking_chunk", chunk)

            # å¢é‡æ‰«æå½“å‰chunkï¼Œå¯»æ‰¾é¦–ä¸ªå¹³è¡¡JSON
            for ch in chunk:
                if not started:
                    if ch == '{':
                        started = True
                        depth = 1
                        json_buffer = ['{']
                    # æœªå¼€å§‹å‰å¿½ç•¥å…¶å®ƒå­—ç¬¦
                    continue

                # å·²å¼€å§‹æ”¶é›†JSON
                json_buffer.append(ch)
                if escape:
                    escape = False
                    continue
                if in_string:
                    if ch == '\\':
                        escape = True
                    elif ch == string_quote:
                        in_string = False
                    continue
                else:
                    if ch == '"' or ch == "'":
                        in_string = True
                        string_quote = ch
                        continue
                    if ch == '{':
                        depth += 1
                    elif ch == '}':
                        depth -= 1
                        if depth == 0:
                            # å®Œæ•´JSONæ”¶é›†å®Œæˆ
                            try:
                                plan_json = json.loads(''.join(json_buffer))
                                final_plan = plan_json
                            except Exception as _:
                                final_plan = {}
                            break
            if final_plan:
                break
        
        # è¿”å›æœ€ç»ˆè§„åˆ’ç»“æœ
        if final_plan:
            yield ("final_plan", final_plan)
        else:
            yield ("final_plan", {
                "reasoning": "LLMå“åº”æ ¼å¼å¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†",
                "action": "direct_response",
                "outline": ["è¯·æ‚¨æä¾›æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¾‹å¦‚å…·ä½“é—®é¢˜æˆ–åœºæ™¯ã€‚"],
                "response": "æŠ±æ­‰ï¼Œæˆ‘åœ¨ç†è§£æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°äº†å›°éš¾ã€‚"
            })
    except Exception as e:
        logger.error(f"LLMè§„åˆ’å¤±è´¥: {e}")
        yield ("final_plan", {
            "reasoning": f"è§„åˆ’è¿‡ç¨‹å‡ºé”™: {str(e)}",
            "action": "direct_response",
            "outline": ["ç¨åè¯·é‡è¯•ï¼Œæˆ–æ¢ä¸ªé—®é¢˜æè¿°ã€‚"],
            "response": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        })


async def _llm_plan_next_action(
    user_input: str, 
    history: str, 
    observations: str,
    planner_model: Optional[str] = None
) -> Dict[str, Any]:
    """ä½¿ç”¨ LLM è§„åˆ’ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼ˆä¿ç•™åŸå§‹éæµå¼æ¥å£ç”¨äºå…¼å®¹ï¼‰"""
    tools_spec = await _get_available_tools_spec()
    prompt = get_agent_planner_prompt(user_input, tools_spec, history, observations)
    
    messages = [
        SystemMessage(content="ä½ æ˜¯äº‘å°ç»çš„è§„åˆ’å™¨ï¼Œå¿…é¡»ä»¥æ ‡å‡†JSONæ ¼å¼å›å¤ã€‚è‹¥ç”¨æˆ·é—®é¢˜æ— éœ€å·¥å…·ï¼Œè¯·é€‰æ‹©direct_responseï¼Œä½†ä¸è¦ç›´æ¥è¾“å‡ºå®Œæ•´å›ç­”ï¼›è¯·ä»¥outlineç»™å‡ºå›ç­”å¤§çº²ï¼ˆ3-6æ¡ï¼‰ã€‚å…è®¸è‡ªç”±é—®ç­”ï¼Œä¸é™äºæ ¡å›­é¢†åŸŸï¼›é™¤éæ¶‰åŠå—é™å†…å®¹ï¼Œå¦åˆ™ä¸è¦æ‹’ç»ã€‚"),
        HumanMessage(content=prompt)
    ]
    
    try:
        # ä½¿ç”¨è¾ƒå¿«æ¨¡å‹è¿›è¡Œè§„åˆ’ï¼ˆè‹¥æœªæŒ‡å®šï¼‰
        model_for_planning = planner_model or "fast"

        result = await run_llm_with_retry(messages, temperature=0.1, model_type=model_for_planning)

        # ä½¿ç”¨å¹³è¡¡æ‹¬å·çš„æ–¹å¼æå–é¦–ä¸ªJSONï¼Œé¿å…æ­£åˆ™å¤§æ–‡æœ¬é˜»å¡
        def _extract_first_json(text: str) -> Optional[str]:
            in_string = False
            string_quote = ""
            escape = False
            depth = 0
            started = False
            buf: list[str] = []
            for ch in text:
                if not started:
                    if ch == '{':
                        started = True
                        depth = 1
                        buf = ['{']
                    continue
                buf.append(ch)
                if escape:
                    escape = False
                    continue
                if in_string:
                    if ch == '\\':
                        escape = True
                    elif ch == string_quote:
                        in_string = False
                    continue
                else:
                    if ch == '"' or ch == "'":
                        in_string = True
                        string_quote = ch
                        continue
                    if ch == '{':
                        depth += 1
                    elif ch == '}':
                        depth -= 1
                        if depth == 0:
                            return ''.join(buf)
            return None

        json_str = _extract_first_json(result or "")
        if json_str:
            return json.loads(json_str)
        else:
            return {
                "reasoning": "LLMå“åº”æ ¼å¼å¼‚å¸¸ï¼Œä½¿ç”¨é»˜è®¤å¤„ç†",
                "action": "direct_response",
                "outline": ["è¯·æ‚¨æä¾›æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¾‹å¦‚å…·ä½“é—®é¢˜æˆ–åœºæ™¯ã€‚"],
                "response": "æŠ±æ­‰ï¼Œæˆ‘åœ¨ç†è§£æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°äº†å›°éš¾ã€‚"
            }
    except Exception as e:
        logger.error(f"LLMè§„åˆ’å¤±è´¥: {e}")
        return {
            "reasoning": f"è§„åˆ’è¿‡ç¨‹å‡ºé”™: {str(e)}",
            "action": "direct_response",
            "outline": ["ç¨åè¯·é‡è¯•ï¼Œæˆ–æ¢ä¸ªé—®é¢˜æè¿°ã€‚"],
            "response": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        }


async def stream_route_and_respond(user_input: str, history: List[Dict] | None = None, model_type: Optional[str] = None) -> AsyncGenerator[str, None]:
    """
    AI Agent æµå¼å¾ªç¯ï¼šè®¡åˆ’-æ‰§è¡Œ-è§‚å¯Ÿ-å†è®¡åˆ’
    
    äº‹ä»¶æ ¼å¼ï¼š
    - {"type": "chain_start", "message": "å¼€å§‹å¤„ç†..."}
    - {"type": "agent_planning", "message": "AIæ­£åœ¨åˆ†æå’Œè§„åˆ’..."}
    - {"type": "planning_progress", "content": "å®æ—¶æ€è€ƒè¿‡ç¨‹..."}
    - {"type": "thought", "content": "æ€è€ƒè¿‡ç¨‹..."}
    - {"type": "plan", "action": "...", "tool_calls": [...]} 
    - {"type": "tool_execution", "tool": "å·¥å…·å", "message": "æ‰§è¡Œå·¥å…·..."}
    - {"type": "observation", "content": "è§‚å¯Ÿç»“æœ...", "result_preview": "..."}
    - {"type": "agent_replanning", "message": "é‡æ–°è§„åˆ’..."}
    - {"type": "final_generation", "message": "ç”Ÿæˆæœ€ç»ˆå›ç­”..."}
    - {"type": "content", "text": "å®é™…å†…å®¹"}
    - {"type": "chain_end", "message": "å®Œæˆ"}
    """
    
    try:
        # æ­¥éª¤1: å¼€å§‹å¤„ç†
        yield json.dumps({
            "type": "chain_start", 
            "message": "æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚..."
        }, ensure_ascii=False) + "\n"

        # é€‰æ‹©é˜¶æ®µæ¨¡å‹ï¼šè‹¥å‰ç«¯æŒ‡å®šé"default"ï¼Œå…¨ç¨‹ä½¿ç”¨ï¼›å¦åˆ™æŒ‰é˜¶æ®µé€‰æ‹©
        use_custom = bool(model_type and model_type != "default")
        planner_model = model_type if use_custom else "fast"
        generation_model = model_type if use_custom else "analysis"

        # å‡†å¤‡å†å²ä¿¡æ¯
        history_text = ""
        if history:
            for msg in history[-5:]:  # é™åˆ¶å†å²é•¿åº¦
                role = msg.get("role", "")
                content = msg.get("content", "")
                history_text += f"{role}: {content}\n"
        
        observations = ""
        max_iterations = 3  # æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯
        iteration = 0
        
        while iteration < max_iterations:
            iteration += 1
            
            # æ­¥éª¤2: AIè§„åˆ’
            yield json.dumps({
                "type": "agent_planning", 
                "iteration": iteration,
                "message": f"",
                "model": planner_model  # æ–°å¢ï¼šæ ‡æ³¨ä½¿ç”¨çš„è§„åˆ’æ¨¡å‹
            }, ensure_ascii=False) + "\n"
            
            # æµå¼è§„åˆ’ï¼Œå®æ—¶è¾“å‡ºæ€è€ƒè¿‡ç¨‹
            plan = {}
            async for event_type, data in _llm_plan_next_action_streaming(user_input, history_text, observations, planner_model):
                if event_type == "thinking_chunk":
                    # è¾“å‡ºå®æ—¶æ€è€ƒè¿‡ç¨‹
                    yield json.dumps({
                        "type": "planning_progress",
                        "content": data
                    }, ensure_ascii=False) + "\n"
                elif event_type == "final_plan":
                    plan = data
                    break
            
            # æ­¥éª¤3: å±•ç¤ºæ€è€ƒè¿‡ç¨‹
            yield json.dumps({
                "type": "thought",
                "content": plan.get("reasoning", "åˆ†æç”¨æˆ·éœ€æ±‚...")
            }, ensure_ascii=False) + "\n"

            # æ­¥éª¤3.5: è¾“å‡ºè§„åˆ’è¯¦æƒ…ï¼ˆè¡ŒåŠ¨ä¸å·¥å…·è°ƒç”¨è®¡åˆ’ï¼‰
            action = plan.get("action", "direct_response")
            tool_calls = plan.get("tool_calls", [])
            outline = plan.get("outline", None)
            yield json.dumps({
                "type": "plan",
                "iteration": iteration,
                "action": action,
                "tool_calls": tool_calls,
                "outline": outline,
                "model": planner_model  # æ–°å¢ï¼šæ ‡æ³¨è§„åˆ’è¾“å‡ºæ‰€ç”¨æ¨¡å‹
            }, ensure_ascii=False) + "\n"
            
            # æ ¹æ®è§„åˆ’æ‰§è¡Œä¸åŒçš„è¡ŒåŠ¨
            if action == "call_tool":
                used_fuzzy = False
                
                for tool_call in tool_calls:
                    tool_name = tool_call.get("tool", "")
                    parameters = tool_call.get("parameters", {})
                    tool_reasoning = tool_call.get("reasoning", "")
                    
                    # æ­¥éª¤4: å·¥å…·æ‰§è¡Œ
                    yield json.dumps({
                        "type": "tool_execution",
                        "tool": tool_name,
                        "parameters": parameters,
                        "message": f"ğŸ”§ æ‰§è¡Œå·¥å…·: {tool_name}",
                        "reasoning": tool_reasoning
                    }, ensure_ascii=False) + "\n"
                    
                    # æ‰§è¡Œå·¥å…·
                    result = await _execute_tool(tool_name, parameters)
                    
                    # æ­¥éª¤5: è§‚å¯Ÿç»“æœ
                    if result.get("success"):
                        # å°†ç»“æœåºåˆ—åŒ–ï¼Œæä¾›é¢„è§ˆå†…å®¹ï¼ˆé¿å…è¿‡é•¿ï¼‰
                        try:
                            result_json = json.dumps(result["result"], ensure_ascii=False)
                        except Exception:
                            result_json = str(result.get("result"))
                        preview = result_json if len(result_json) <= 1200 else (result_json[:1200] + "...")
                        observation = f"å·¥å…· {tool_name} æ‰§è¡ŒæˆåŠŸï¼Œç»“æœ: {preview}"
                        yield json.dumps({
                            "type": "observation",
                            "tool": tool_name,
                            "success": True,
                            "content": f"âœ… {tool_name} æ‰§è¡ŒæˆåŠŸ",
                            "result_preview": preview
                        }, ensure_ascii=False) + "\n"
                    else:
                        error_msg = result.get('error', 'æœªçŸ¥é”™è¯¯')
                        observation = f"å·¥å…· {tool_name} æ‰§è¡Œå¤±è´¥: {error_msg}"
                        yield json.dumps({
                            "type": "observation", 
                            "tool": tool_name,
                            "success": False,
                            "content": f"âŒ {tool_name} æ‰§è¡Œå¤±è´¥: {error_msg}",
                            "error": error_msg
                        }, ensure_ascii=False) + "\n"
                        
                    observations += observation + "\n"
                    if tool_name.startswith("fuzzy_search"):
                        used_fuzzy = True
                
                # åˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­è§„åˆ’
                if used_fuzzy and iteration < max_iterations:
                    # å¦‚æœæ‰§è¡Œäº†æ¨¡ç³Šæœç´¢ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥çš„ç²¾ç¡®æŸ¥è¯¢
                    yield json.dumps({
                        "type": "agent_replanning",
                        "message": "ğŸ”„ åŸºäºæœç´¢ç»“æœï¼Œé‡æ–°è§„åˆ’ä¸‹ä¸€æ­¥..."
                    }, ensure_ascii=False) + "\n"
                    continue
                else:
                    # æœ‰äº†è¶³å¤Ÿä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆå›ç­”
                    break
            
            elif action == "direct_response":
                # ä¸ç›´æ¥è¾“å‡ºæœ€ç»ˆå›ç­”ï¼›è¿›å…¥ç»Ÿä¸€æœ€ç»ˆç”Ÿæˆé˜¶æ®µ
                break
            
            else:
                observations += f"æœªçŸ¥è¡ŒåŠ¨ç±»å‹: {action}\n"
                break
        
        # æ­¥éª¤6: ç”Ÿæˆæœ€ç»ˆå›ç­”
        yield json.dumps({
            "type": "final_generation",
            "message": "ç”Ÿæˆæœ€ç»ˆå›ç­”...",
            "model": generation_model  # æ–°å¢ï¼šæ ‡æ³¨ç”¨äºæœ€ç»ˆç”Ÿæˆçš„æ¨¡å‹
        }, ensure_ascii=False) + "\n"

        # ç»Ÿä¸€æœ€ç»ˆç”Ÿæˆé€»è¾‘ï¼šèåˆå¤§çº²/è‰æ¡ˆ/è§‚å¯Ÿç»“æœ
        planner_outline = (plan.get("outline") or [])
        planner_response_hint = plan.get("response", "")
        # ç»Ÿä¸€ä¸Šä¸‹æ–‡
        context_parts = [
            f"ç”¨æˆ·é—®é¢˜: {user_input}"
        ]
        if planner_outline:
            bullets = "\n".join(f"- {item}" for item in planner_outline if item)
            context_parts.append(f"è§„åˆ’å™¨æä¾›çš„å¤§çº²:\n{bullets}")
        if planner_response_hint:
            context_parts.append(f"è§„åˆ’å™¨çš„ä¸€å¥è¯è‰æ¡ˆ:\n{planner_response_hint}")
        if observations.strip():
            context_parts.append(f"è·å¾—çš„ä¿¡æ¯ï¼ˆå·¥å…·/æœç´¢è§‚å¯Ÿï¼‰:\n{observations.strip()}")
        context_parts.append("è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆç»“æ„æ¸…æ™°ã€ç®€æ´æœ‰ç”¨çš„æœ€ç»ˆå›ç­”ã€‚è‹¥ä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜å¹¶ç»™å‡ºä¸‹ä¸€æ­¥å»ºè®®ã€‚")
        context = "\n\n".join(context_parts)

        messages = [
            SystemMessage(content="ä½ æ˜¯æ ¡å›­æ™ºèƒ½åŠ©æ‰‹â€œäº‘å°ç»â€ã€‚è¯·åŸºäºè§„åˆ’å¤§çº²ä¸å¯ç”¨ä¿¡æ¯ç”Ÿæˆæœ€ç»ˆå›ç­”ã€‚å…è®¸è‡ªç”±é—®ç­”ï¼Œä¸é™äºæ ¡å›­é¢†åŸŸï¼›é™¤éæ¶‰åŠå—é™å†…å®¹ï¼Œå¦åˆ™ä¸è¦æ‹’ç»ã€‚"),
            HumanMessage(content=context)
        ]
        async for chunk in stream_chat_response(messages, temperature=0.5, model_type=generation_model):
            yield json.dumps({"type": "content", "text": chunk}, ensure_ascii=False) + "\n"

        # æ­¥éª¤8: ç»“æŸæ ‡è¯†
        yield json.dumps({
            "type": "chain_end",
            "message": "å®Œæˆ"
        }, ensure_ascii=False) + "\n"
        
    except Exception as e:
        logger.error(f"Agentå¾ªç¯å‡ºé”™: {e}")
        yield json.dumps({
            "type": "error",
            "message": f"âŒ å¤„ç†å‡ºé”™: {str(e)}"
        }, ensure_ascii=False) + "\n"

